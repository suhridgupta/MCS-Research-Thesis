\clearpage

\def\chaptertitle{Introduction}

\lhead{\emph{\chaptertitle}}

\chapter{\chaptertitle}
\label{ch:introduction}

In this chapter, a brief overview of the edge computing architecture paradigm, along with its uses, benefits, and challenges with respect to resource scaling are provided in section \ref{sec:edge-arch}. These challenges lead to the research gap and questions this thesis intends to answer in section \ref{sec:problem-overview}.\par

\section{Edge Computing Overview}
\label{sec:edge-arch}

Cloud computing architectures leverage the on-demand accessibility of the Internet. The cloud applications utilize the vast resources of the cloud to perform a task, and relinquish it once it is complete for the other sub-modules in the application to request \cite{rimal2009taxonomy}. In the early days, a singular end-point would be used to access these services, however nowadays the architecture is multi-regional allowing effortless access from across the world. This was achieved through the use of content delivery networks (CDN) located in several regions to allow for data to be quickly replicated and served to clients. This architecture model allows for the processing of large-scale data in a near real-time manner.\par

During the early twenty-first century, this architecture paradigm dominated the Information Technology (IT) industry. Compared to traditional monolithic architectures, the ease of deployment, scalability, coupled with the economic benefits ensured its dominance. The increasing popularity of hand-held devices as well as home appliances has resulted in data being largely produced at the edge of the cloud network. Thus, processing this large amount of data solely on the cloud proved to be an inefficient solution due to the bandwidth limitations of the network \cite{shi2016edge}. Thus, edge computing paradigms were built on the previous foundation of CDNs \cite{satyanarayanan2017emergence}. Edge computing architectures ensure data processing services and resources exist at the peripheries of the network \cite{cao2020overview}. The architecture extends and adapts the computing and networking capabilities of the cloud to meet real-time, low latency, and high bandwidth requirements of modern agile businesses.\par

Edge computing deploys several lightweight computing devices known as cloudlets to form a ``mini-cloud'' and places them in close proximity to the end-user data \cite{liu2019survey}. This reduces the latency in terms of client-server communication and data processing. Cloudlets can also be easily scaled depending on the resource requirements per edge architecture \cite{ren2019survey}. However, due to the dynamic resource requirements which may fluctuate from time to time, the resources allocated to cloudlets must be dynamically scaled too. This dynamic scaling, along with the inherent latency present between the cloud layer and the edge cloudlets, poses a significant problem to real-time resource scaling \cite{varghese2016challenges}.\par
One method of mitigating this scaling latency is through the use of micro-service applications. By employing a microservice architecture, the resources in a cloudlet are a collection of smaller deployments that are both independent and loosely coupled \cite{villamizar2015evaluating}. This loose coupling ensures that parts of the cloudlet can be scaled as is required, further reducing the time required to scale resources as compared to scaling the cloudlet monolithically.\par

The scaling of these microservice resources is done automatically through a process known as autoscaling. While most microservice applications come bundled with default autoscaling solutions, and these solutions are sufficient for most applications, they fall apart when scaling resources for time-sensitive services such as the ones used in healthcare require stringent compliance to service level agreements (SLA) on metrics such as application latency. This has led to further research on autoscaling solutions for edge computing applications. These primarily fall into two categories. Reactive autoscaling solutions attempt to modify the microservice resource allocation once the required resources exceeds the current allocation. These algorithms are simple to develop and deploy, however the time taken to scale resources leads to a degradation of resource availability and violates SLA compliance \cite{podolskiy2018iaas}. To counteract these pitfalls, proactive autoscaling solutions attempt to model the resource allocation over time and effectively predict the resource requirements. By doing so, the microservice resources can be scaled in advance through a process known as ``cold starting''. This approach removes the latency inherent in scaling resources, however the algorithms are extremely complex to develop, train as well as tune to specific edge applications \cite{straesser2022not}.

\section{Problem Overview}
\label{sec:problem-overview}

With these limitations in mind, the object of this research project was to unify the reactive and proactive autoscaling solutions into a hybrid model. This solution combines the light weight, and ease of use and deployment of the reactive algorithm, with the SLA compliance and accurate resource modelling of proactive ones. The project aimed to answer the following questions in a satisfactory manner:
\begin{itemize}
    \item \textbf{\textit{RQ1:}} Can we integrate reactive and proactive autoscaling methods to develop a tailored algorithm for edge computing that eliminates the requirement for developers to fine-tune hyper-parameters for each autoscaling use case, while being lightweight enough to be deployed and run on cloudlets?
    \item \textbf{\textit{RQ2:}} Can the hybrid autoscaling solution achieve or exceed the SLA compliance capabilities of state-of-the-art reactive and proactive autoscaling solutions for edge computing, while minimizing the degredation of application performance?
\end{itemize}